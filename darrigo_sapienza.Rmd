---
title: "SDS HOMEWORK 2"
author: "Stefano D'Arrigo & Jeremy Sapienza"
date: "11 gennaio 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Whack a MoG

In this exercise we design a simulation study in order to compare the performance of different model selection techniques.

## 1. Code for the EM-fit of a generic mixture of $k \geq 1$ Gaussians
As a first step for this study, we extend the code provided [here](https://tinyurl.com/y9lgxbdz) in order to work with an arbitrary number of Gaussian components.
Before modifying the `handmade.em` function, we define some routines which we make use of within it.


The first routine, `d.init`, computes the E step of the EM algorithm.

$$~$$
```{r}
d.init <- function(y, p, mu, sigma) {
  k <- length(p) # save the number of K gaussian components which is equal to the length of weights' vector                           
  len.y <- length(y) # save the number of samples
  d <- matrix(NA, k, len.y) # define the matrix with k components as number of rows and len.y as number of columns
  
  for(i in 1:k) {
    d[i,] <- p[i] * dnorm(y, mu[i], sigma[i]) # for each k iteration we update the probability for each y observation 
  }
  
  return(d)
}
```
$$~$$

The `likelihood` function is another routine used into the `handmade.em` that calculates for each observation the corresponding likelihood probability.

```{r}
likelihood <- function(y, p, mu, sigma) {
  kk <- length(p) # save the number of K gaussian components which is equal to the length of weights' vector
  like <- 0
  
  for(i in 1:kk) {
    # for each observation calculates the p(y|x;theta) where theta are the parameters of the Gaussian 
    like <- like + p[i] * dnorm(y, mu[i], sigma[i]) 
  }
  
  return(like)
}
```

$$~$$

With this last support routine for the `handmade.em` function wrote below, we assign to each observation the color of the gaussian component which it most likely belongs to. 

```{r message=FALSE, warning=FALSE}
require(ramify) # we import this library in order to apply the argmax through the d matrix

assign.color <- function(cols, d) {
  argm <- argmax(d, rows = FALSE) # we find the argmax through each column
  return(cols[argm]) # return the corresponding color
}
```

$$~$$

At this point the `handmade.em` function generalized for an arbitrary number of Gaussian components: 

```{r message=FALSE, warning=FALSE}
require(randomcoloR) # we import this library in order to generate k random colors

handmade.em <- function(y, p, mu, sigma, n.iter, plot.flag = T)
{
  k <- length(p) # save the number of K gaussian components which is equal to the length of weights' vector
  len.y <- length(y) # save the number of samples
  
  # set k random colors
  cols <- randomColor(count = k, 
                      hue = c(" ", "random", "red", "orange", "yellow", "green", "blue", "purple", "pink", "monochrome"), 
                      luminosity = c(" ", "random", "light", "bright", "dark"))
  
  # compute the likelihood for k Gaussians
  like <- likelihood(y, p, mu, sigma)
  
  deviance <- -2 * sum(log(like))
  res      <- matrix(NA,n.iter + 1, 3 * k + 2) # the columns are 3*k because 3 are the Gaussian parameters of length k and +2 because we want to save also the number of each iteration and the corresponding deviance
  res[1,]  <- c(0, p, mu, sigma, deviance) # here we store the result for each i-th iteration
  
  
  d <- d.init(y, p, mu, sigma) # E step with the initial parameters
  r <- matrix(NA, k, len.y) # initialization of the responsabilities matrix 
  for (iter in 1:n.iter) { # start i-th iteration
    
    # E step - computes the responsabilites for each observation
    for(i in 1:k) {
      r[i,] <- d[i,] / colSums(d)
    }
    
    # M step - update the theta parameters for the Gaussian components
    for(i in 1:k) {
      p[i] <- mean(r[i,])
      mu[i] <- sum(r[i,] * y) / sum(r[i,])
      sigma[i] <- sqrt(sum(r[i,] * (y^2)) / sum(r[i,]) - (mu[i])^2)
    }
    
    like <- likelihood(y, p, mu, sigma)
    
    # -2 x log-likelihood (a.k.a. deviance)
    deviance <- -2 * sum( log(like) )
    
    # Save the results of the i-th iteration
    res[iter+1, ] <- c(iter, p, mu, sigma, deviance)
    
    # E step - update probabilities matrix
    d <- d.init(y, p, mu, sigma) 
    
    # Plot
    if (plot.flag){
      hist(y, prob = T, breaks = 50, col = gray(.8), border = NA, 
           main = "", xlab = paste("EM Iteration: ", iter, "/", n.iter, sep = ""))
      set.seed(123)
      points(jitter(y), rep(0,length(y)), 
             pch = 19, cex = .6, 
             col = assign.color(cols, d)) # assigns the colors to the observation point
      curve(likelihood(x, p, mu, sigma),
            lwd = 4, col = rgb(0,0,0,.5), add = TRUE)
      Sys.sleep(1.5)
    }
  }
  
  res <- data.frame(res)
  names(res) <- c("iteration", paste("p", 1:k, sep=""), paste("mu", 1:k, sep=""), paste("sigma", 1:k, sep=""), "deviance") # generalize the number of columns with k components parameters
  out <- list(parameters = c(p = p, mu = mu, sigma = sigma), deviance = deviance, res = res) # return the final result
  return(out)
}
```

# Test EM-fit with different sample sizes

We pick two different sample sizes $n_1 < n_2$, in order to study the behavior of the algorithm in a clearly non-asymptotic regime and in a reasonably asymptotic one.

We define the following function in oder to random initialize parameters of the model

```{r}
initialize.parameters <- function(y, k) {
  p <- rep(1/k, k) # equally weight the k components
  mu    <- runif(k, min = 0.1, max= 0.9) # generate random k means parameters
  sigma <- runif(k, min = 0.1, max= 0.9) # generate random k sigma parameters
  
  return(list(p = p, mu = mu, sigma = sigma)) # return the list of generated parameters 
}
```

The data we are going to use come from the Bart distribution which is computed by the following function

```{r message=FALSE, warning=FALSE}
require(mixtools) # import the library in order to generate the MoG

bart.samples <- function(n) {
  return( rnormmix(n,
                   lambda = c(0.5, rep(0.1,5)),
                   mu = c(0, ((0:4)/2)-1),
                   sigma = c(1, rep(0.1,5)) ))
}
```

```{r include=FALSE}
plot.curve <- function(y, p, mu, sigma, n.iter) { # useful to plot the final result iteration
  cols <- randomColor(count = length(p), 
                    hue = c(" ", "random", "red", "orange", "yellow", "green", "blue", "purple", "pink", "monochrome"), 
                    luminosity = c(" ", "random", "light", "bright", "dark"))
  d <- d.init(y, p, mu, sigma)
    
  hist(y, prob = T, breaks = 50, col = gray(.8), border = NA, 
     main = "", xlab = paste("EM Iteration: ", n.iter, sep = ""))
  set.seed(123)
  points(jitter(y), rep(0,length(y)), 
         pch = 19, cex = .6, 
         col = assign.color(cols, d))
  curve(likelihood(x, p, mu, sigma),
        lwd = 4, col = rgb(0,0,0,.5), add = TRUE)
}
```

## Non-Asymptotic regime

```{r}
n1 <- 150 # Sample size that follows a non-asymptotic way
XX <- bart.samples(n1)

parameters <- initialize.parameters(XX, k=6) # k is equal to six according to the bart distribution

#extract the corresponding gaussian parameters
p <- parameters$p 
mu <- parameters$mu 
sigma <- parameters$sigma

# executed the handmade.em function for twenty iterations
out1 <- handmade.em(XX, p, mu, sigma, n.iter = 20, plot.flag = F)
plot.curve(XX, out1$parameters[1:6], out1$parameters[7:12], out1$parameters[13:18], n.iter = 20)
```

## Asymptotic regime

```{r}
n2 <- 2000 # Sample size that follows a reasonably asymptotic way
XX <- bart.samples(n2)

parameters <- initialize.parameters(XX, k=6) # k is equal to six according to the bart distribution

#extract the corresponding gaussian parameters
p <- parameters$p 
mu <- parameters$mu 
sigma <- parameters$sigma

# executed the handmade.em function for twenty iterations
out2 <- handmade.em(XX, p, mu, sigma, n.iter = 20, plot.flag = F)
plot.curve(XX, out2$parameters[1:6], out2$parameters[7:12], out2$parameters[13:18], n.iter = 20)
```

<!-- add a out1 and out2 plots -->

## The reasons of these results?

Our initial expectations are confirmed. With the Law of Large Numbers and the Central Limit Theorem the number of samples are significant in order to follow the true distribution. The LLN rule says that we can achieve the real first moment distribution increasing the number of samples and with the CLT we can see if the distribution follows the Gaussian shape.

$$~$$

# Model Selection

In this section we introduce different methods in order to find the best k components for these particular sample sizes distributed from a bart density.

## AIC (Akaike Information Criterion) & BIC (Bayesian Information Criterion)

The AIC method is used for the model selection, it adds a penalization proportional to the complexity of the models: the more complex the model is the more relevant is the penalization.

$$AIC = 2 \cdot l_j(\hat\theta_j) - 2 \cdot dim(\Theta_j)$$

where:

- $\hat\theta$  is the vector of estimated parameters
- $l_j$ is the likelihood function
- $dim(\Theta)$ is the number of the model parameters

As AIC, the BIC method is used for the model selection. It penalizes more the model when the model complexity increases.

$$BIC = l_j(\hat\theta_j) - \frac{\log(n)}{2} \cdot dim(\Theta_j)$$

where:

- $\hat\theta$  is the vector of estimated parameters
- $l_j$ is the likelihood function
- $dim(\Theta)$ is the number of the model parameters
- $n$ is the sample size

As defined below, we implement a general function in order to choose which penalty apply to the model:

$$~$$

```{r}
get.AIC.BIC <- function(y, kmax, n.iter) {
  len.y <- length(y);
  aic.score <- c(); bic.score <- c()
  
  for (k in 1:kmax){
    parameters <- initialize.parameters(y, k) # randomly initialize parameters
    p <- parameters$p
    mu <- parameters$mu
    sigma <- parameters$sigma 
    
    out.opt <- handmade.em(y, p, mu, sigma, n.iter = n.iter, plot.flag = F)$parameters # get the estimated parameters
    
    p <- out.opt[1:k]
    mu <- out.opt[(k+1):(2*k)]
    sigma <- out.opt[(2*k+1):(3*k)]
      
    like <- likelihood(y, p, mu, sigma) # calculate the likelihood with the previous estimated parameters
  
    aic.score <- c(aic.score, 2 * sum(log(like)) - 2 * (3*k-1)) # calculate the AIC #### DESCRIBE WELL
    bic.score <- c(bic.score, sum(log(like)) - log(len.y)/2 * (3*k-1))
  }
  res <- list()
  res[["best_AIC"]] <- which.max(aic.score) # return the index of the k component the maximize the score 
  res[["best_BIC"]] <- which.max(bic.score)
  return(res)
}
```

$$~$$

## Sample Splittings

This particular method divides randomly the main dataset into training and test set given a percentage of this splits. After that, we estimate the model parameters using the training set and we evaluate the "goodness" of the estimations using the test set.

$$~$$

```{r}
split.data <- function(y, perc) {
  size.s <- floor(perc * length(y)) # catch the number of observations at x%
  trainIndex <- sample(seq_len(length(y)), size = size.s) # randomly selection of n observations
  return (trainIndex)
}

training.model <- function(y.train, y.test, kmax, n.iter) {
  like.j <- c()
  
  for (k in 1:kmax){
    parameters <- initialize.parameters(y.train, k) # randomly initialize parameters
    p <- parameters$p 
    mu <- parameters$mu 
    sigma <- parameters$sigma
    
    out.opt <- handmade.em(y.train, p, mu, sigma, n.iter = n.iter, plot.flag = F)$parameters # get the estimated parameters
    
    p <- out.opt[1:k]
    mu <- out.opt[(k+1):(2*k)]
    sigma <- out.opt[(2*k+1):(3*k)]
    
    like <- likelihood(y.test, p, mu, sigma) # calculate the likelihood with the previous estimated parameters
    
    like.j <- c(like.j, sum(log(like)))
  }
  
  best.k <- which.max(like.j) # return the index of the k component the maximize the score 
  return(best.k)
}
```

$$~$$

## K-Fold Cross-Validation

This method used in model selection randomly divides the observations into k folds of approximately equal size. This choose one fold as test set and the other ones are used as training set in order to fit the model.

$$~$$

```{r message=FALSE, warning=FALSE}
library(caret) # import the dataset in order to crate the foldes

k.fold.cross.validation <- function(y, k.folds, kmax, n.iter) {
  best.cross.v <- c() 
  expectations <- c()
  like.j <- c()
  
  lst.k.folds <- createFolds(y, k = k.folds, list = TRUE, returnTrain = FALSE) # create k folds with n observations
  
  fold.names <- names(lst.k.folds)
  for (k in 1:kmax) { # train the model kmax iterations
    for (x in 1:k.folds) { # train each fold k times
      # divide the observations
      n.k.fold <- fold.names[x]
      
      test.set <- y[unlist(lst.k.folds[n.k.fold], use.names=FALSE)]            
      train.set <- y[-unlist(lst.k.folds[n.k.fold], use.names = FALSE)]          
      
      # randomly initialize parameters
      parameters <- initialize.parameters(train.set, k) 
      p <- parameters$p 
      mu <- parameters$mu 
      sigma <- parameters$sigma
      
      # get the estimated parameters
      out.opt <- handmade.em(train.set, p, mu, sigma, n.iter = n.iter, plot.flag = F)$parameters
      
      p <- out.opt[1:k]
      mu <- out.opt[(k+1):(2*k)]
      sigma <- out.opt[(2*k+1):(3*k)]
      
      # calculate the likelihood with the previous estimated parameters
      like <- likelihood(test.set, p, mu, sigma)  
      
      # for each fold calculate the mean of the log-likelihood
      like.j <- c(like.j, mean(log(like)))
    }
    
    # save the i-th expectation
    expectations <- c(expectations, mean(like.j))
  }

  best.cross.v <- which.max(expectations) # return the index of the k component the maximize the score
  return(best.cross.v)
}
```

$$~$$

## Wasserstein

The Wasserstein distance is a distance function defined between probability distributions on a given metric space. For our work we evaluate this formula:

$$W_k = \int_0^1 |{F_k^{-1}(z|\hat\theta_{Tr})-\hat F_{Te}^{-1}(z)}| dz$$


where:

- $F_k^{-1}(z|\hat\theta_{Tr})$ is the quantile function of a k components MoG
- $F_{Te}^{-1}(z)$ denotes the quantile function of the true population model

$$~$$

```{r message=FALSE, warning=FALSE}
library(KScorrect) # import this library in order to apply the integrate function

wass.score <- function(y.train, y.test, kmax, n.iter) { # routine function useful to apply the Wasserstein formula
  f <- function(z, p, mu, sigma, y.test) {
    res <- abs(qmixnorm(p = z, mean = mu, sd = sigma, pro = p) - quantile(y.test, probs = z))
    return(res)
  }
  
  wass.sc <- c();
  
  for (k in 1:kmax){
    parameters <- initialize.parameters(y.train, k) # randomly initialize parameters
    p <- parameters$p 
    mu <- parameters$mu 
    sigma <- parameters$sigma
    
    out.opt <- handmade.em(y.train, p, mu, sigma, n.iter = n.iter, plot.flag = F)$parameters # get the estimated
    
    p <- out.opt[1:k]
    mu <- out.opt[(k+1):(2*k)]
    sigma <- out.opt[(2*k+1):(3*k)]
    
    wass.sc <- c(wass.sc, integrate(f, lower = 0, upper = 1,
                                    p = p, mu = mu, sigma = sigma, y.test = y.test, 
                                    rel.tol=.Machine$double.eps^.05)$value) # execute the integral in order to have the score
  }
  
  best.wass <- wass.sc # return the index of the k component the minimize the score
  return(best.wass)
}
```

$$~$$

# ...Do Simulations!

We define a routine function in order to simulate M times the samples from a bart density. 

$$~$$

```{r}
do.simulate <- function(n, M, n.iter, kmax) {
  max.k <- function(res) { # return the max indexes of k components
    k <- 0
    max <- -1
    for(i in 1:length(res)) {
      if(max <= res[i] && k < i) {
        k <- i
        max <- res[i]
      }
    }
    return(k)
  }

  #initialization of parameters
  best.score50and50 <- c(); best.score70and30 <- c(); best.score30and70 <- c();
  best.aic.k <- c(); best.bic.k <- c();
  best.cross.validation5 <- c(); best.cross.validation10 <- c();
  
  wasses <- c(); 
  
  wasses.scores <- data.frame(matrix(ncol=2,nrow=0, dimnames=list(NULL, c("k.component", "score"))))
  
  # M simulations starting now..
  for (i in 1:M) {
    XX <- bart.samples(n)
    
    # find the best k components with AIC
    res.AIC.BIC <- get.AIC.BIC(XX, kmax = kmax, n.iter = n.iter)
    best.aic.k <- c(best.aic.k, res.AIC.BIC[["best_AIC"]])
    
    best.bic.k <- c(best.bic.k, res.AIC.BIC[["best_BIC"]])
    
    # 50% Training-Test
    indexes50.50 <- split.data(XX, perc=0.5)
    XX.Train50 <- XX[indexes50.50]
    XX.Test50 <- XX[-indexes50.50]
    
    best.score50and50 <- c(best.score50and50, training.model(XX.Train50, XX.Test50, kmax=kmax, n.iter))
    
    # 70%-30% Training-Test
    indexes70.30 <- split.data(XX, perc=0.7)
    XX.Train70 <- XX[indexes70.30]
    XX.Test30 <- XX[-indexes70.30]
    
    best.score70and30 <- c(best.score70and30, training.model(XX.Train70, XX.Test30, kmax=kmax, n.iter))
    
    # 30%-70% Training-Test
    indexes30.70 <- split.data(XX, perc=0.3)
    XX.Train30 <- XX[indexes30.70]
    XX.Test70 <- XX[-indexes30.70]
    
    best.score30and70 <- c(best.score30and70, training.model(XX.Train30, XX.Test70, kmax=kmax, n.iter))
    
    # K-Fold Cross-Validation
    best.cross.validation5 <- c(best.cross.validation5, k.fold.cross.validation(XX, k.folds=5, kmax=kmax, n.iter))
    best.cross.validation10 <- c(best.cross.validation10, k.fold.cross.validation(XX, k.folds=10, kmax=kmax, n.iter))
    
    #Wass score
    wasses.scores <- rbind(wasses.scores, data.frame( k.component = 1:kmax, 
                                                      score = wass.score(XX.Train50, XX.Test50, kmax=kmax, n.iter) ) )
    wasses <- c(wasses, which.min(wass.score(XX.Train50, XX.Test50, kmax=kmax, n.iter)))
  }
  
  return(list(  AIC = best.aic.k, 
                BIC = best.bic.k,
                Split50_50 = best.score50and50,
                Split70_30 = best.score70and30,
                Split30_70 = best.score30and70,
                five_fold_cv = best.cross.validation5,
                ten_fold_cv = best.cross.validation10,
                wass_score_k = wasses,
                wasses_scores = wasses.scores
              )
         )
  
  # return( data.frame( AIC = max.k(tabulate(best.aic.k)), 
  #                     BIC = max.k(tabulate(best.bic.k)),
  #                     Split50_50 = max.k(tabulate(best.score50and50)),
  #                     Split70_30 = max.k(tabulate(best.score70and30)),
  #                     Split30_70 = max.k(tabulate(best.score30and70)),
  #                     five_fold_cv = max.k(tabulate(best.cross.validation5)),
  #                     ten_fold_cv = max.k(tabulate(best.cross.validation10)),
  #                     wass_score = max.k(tabulate(wasses)),
  #                     
  #                     row.names = "kmax")
  #)
}
```

$$~$$

Start to simulate with n1 and n2 samples size defined above.

```{r}
kmax <- 8

# try to simulate with n1 = 150 
res.n1 <- do.simulate(n1, M=50, n.iter=20, kmax=kmax)

# try to simulate with n1 = 2000 
res.n2 <- do.simulate(n2, M=50, n.iter=20, kmax=kmax)
```

$$~$$

# Final comments

<!-- describe the results and gives the final comments -->

<!-- Remark the previous results about non-asymptotic and asymptotic regime -->

<!--
PLOT & SUMMARY
  for the best kmax plot the bart distro
-->

```{r}
train.model <- function(y, n.iter, kmax) {
  out <- c(rep(0, kmax))
  for (k in 1:kmax){
    parameters <- initialize.parameters(y.train, k) # randomly initialize parameters
    p <- parameters$p 
    mu <- parameters$mu 
    sigma <- parameters$sigma
    
    out[k] <- handmade.em(y.train, p, mu, sigma, n.iter = n.iter, plot.flag = F) # get the estimated
  }
  return(out)
}
```

# Plot & summaries

We are focussing on each iteration for each model that we picked in order to display the k components in order to the proper frequency of it:

```{r include=FALSE}
plot.AIC.BIC <- function(res, n) {
  freq.aic <- tabulate(res[["AIC"]])
  freq.bic <- tabulate(res[["BIC"]])
  plot(1:length(freq.aic), freq.aic,
       main = paste("K distribution with ", n, " samples across M=100 simulations", sep=""), xlab="k components", ylab="Frequency",
       lty=3, lwd = 3, type = "b", pch = 21, col="red", xlim = c(1, kmax), ylim = c(1, 100))
  lines(1:length(freq.bic), freq.bic,
        lty=3, lwd = 3, type = "b", pch = 21, col="blue")
  legend(7, 95, legend=c("AIC", "BIC"),
         col=c("red", "blue"), lty=2, cex=0.8) 
}

plot.Splittings <- function(res, n) {
  freq.Split50_50 <- tabulate(res[["Split50_50"]])
  freq.Split70_30 <- tabulate(res[["Split70_30"]])
  freq.Split30_70 <- tabulate(res[["Split30_70"]])
  
  plot(1:length(freq.Split50_50), freq.Split50_50,
       main = paste("K distribution with ", n, " samples across M=100 simulations", sep=""), xlab="k components", ylab="Frequency",
       lty=3, lwd = 3, type = "b", pch = 21, col="red", xlim = c(1, kmax), ylim = c(1, 100))
  lines(1:length(freq.Split70_30), freq.Split70_30,
       lty=3, lwd = 3, type = "b", pch = 21, col="blue")
  lines(1:length(freq.Split30_70), freq.Split30_70,
       lty=3, lwd = 3, type = "b", pch = 21, col="green")
  legend(6.3, 95, legend=c("Split 50-50%", "Split 70-30%", "Split 30-70%"),
         col=c("red", "blue", "green"), lty=2, cex=0.8)
}

plot.kfolds <- function(res, n) {
  freq.five_fold_cv <- tabulate(res[["five_fold_cv"]])
  freq.ten_fold_cv <- tabulate(res[["ten_fold_cv"]])
  
  plot(1:length(freq.five_fold_cv), freq.five_fold_cv,
       main = paste("K distribution with ", n, " samples across M=100 simulations", sep=""), xlab="k components", ylab="Frequency",
       lty=3, lwd = 3, type = "b", pch = 21, col="red", xlim = c(1, kmax), ylim = c(1, 100))
  lines(1:length(freq.ten_fold_cv), freq.ten_fold_cv,
       lty=3, lwd = 3, type = "b", pch = 21, col="blue")
  legend(6.5, 95, legend=c("5-Fold", "10-Fold"),
         col=c("red", "blue"), lty=2, cex=0.8)  
}

plot.wass <- function(res, n) {
  freq.wass <- tabulate(res[["wass_score_k"]])
  
  plot(1:length(freq.wass), freq.wass,
       main = paste("K distribution with ", n, " samples across M=100 simulations", sep=""), xlab="k components", ylab="Frequency Wass Score",
       lty=3, lwd = 3, type = "b", pch = 21, col="red", xlim = c(1, kmax), ylim = c(1, 50))
}

violin.plot <- function(res, n) {
  res$wasses_scores$k.component <- as.factor(res$wasses_scores$k.component)
  p <- ggplot(res$wasses_scores, aes(x=k.component, y=score)) + geom_violin(trim=FALSE, aes(fill=k.component)) +
       geom_boxplot(width=0.1) + 
       labs(title=paste("Plot of k-component by scores with ", n, " samples"),x="k component", y = "scores") + 
       scale_fill_brewer(palette="Dark2") + 
       theme_minimal()
  p 
}
```

```{r}
# Show plots of k components with the proper results with n1 and n2 samples
plot.AIC.BIC(res.n1, "n1")
plot.Splittings(res.n1, "n1")
plot.kfolds(res.n1, "n1")
plot.wass(res.n1, "n1")

plot.AIC.BIC(res.n2, "n2")
plot.Splittings(res.n2, "n2")
plot.kfolds(res.n2, "n2")
plot.wass(res.n2, "n2")
```

We want for plotting the wasses scores another type of representation with violin plot similar to box plot, but the main difference is the presence of the distribution of these data:

```{r}
violin.plot(res.n1, "n1")
violin.plot(res.n2, "n2")
```

Now, We want to see which model has predicted really well the true kmax for the bart distribution considering kmax_true = 6:

```{r include=FALSE}
best.model.prediction <- function(res, n) {
  kmax_true = 6; scores <- c();
  M <- length(res$AIC) # the length of simulations
  for (name in names(res)) {
    if (name!="wasses_scores") {
      perc.pred <- sum(res[[name]]==kmax_true) / M
      scores <- c(scores, perc.pred)
      
      print(paste(name, ": ", perc.pred))
    }
  }
  
  print(paste("----> The best model for predicting the true kmax with", n, "samples is: ", names(res)[which.max(scores)]))
}

best.model.prediction(res.n1, "n1")
best.model.prediction(res.n2, "n2")
```


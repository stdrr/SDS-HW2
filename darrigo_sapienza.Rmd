---
title: "SDS HOMEWORK 2"
author: "Stefano D'Arrigo & Jeremy Sapienza"
date: "11 gennaio 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(cache = TRUE)
```

# Whack a MoG

In this exercise we design a simulation study, considering the Bart distribution as data and training a Mixture of Gaussians model through the Expectation-Maximization optimization algorithm. Different techniques are taken into account in order to select the model.

$$~$$

## 1. Code for the EM-fit of a generic mixture of $k \geq 1$ Gaussians

As a first step for this study, we extend the code provided [here](https://tinyurl.com/y9lgxbdz) in order to work with an arbitrary number of Gaussian components.
Before modifying the `handmade.em` function, we define some routines which we make use of within it.

$~$

First, the `likelihood` function is a routine used into the `handmade.em` and calculates, for each observation, the corresponding likelihood probability.

```{r}
likelihood <- function(y, p, mu, sigma) {
  k <- length(p) # save the number of K gaussian components which is equal to the length of weights' vector
  like <- 0
  
  for(i in 1:k) {
    # for each observation calculates the p(y|x;theta) where theta are the parameters of the Gaussian 
    like <- like + p[i] * dnorm(y, mu[i], sigma[i]) 
  }
  
  return(like)
}
```

$$~$$

With this last support routine, we assign to each observation the color of the Gaussian component which it most likely belongs to. 

```{r message=FALSE, warning=FALSE}
require(ramify) # we import this library in order to apply the argmax through the d matrix

assign.color <- function(cols, d) {
  argm <- argmax(d, rows = FALSE) # we find the argmax through each column
  return(cols[argm]) # return the corresponding color
}
```

$$~$$

At this point the `handmade.em` function generalized for an arbitrary number of Gaussian components is: 

```{r message=FALSE, warning=FALSE}
require(randomcoloR) # we import this library in order to generate k random colors

handmade.em <- function(y, p, mu, sigma, n.iter, plot.flag = T, tol = 0.0001)
{
  k <- length(p) # save the number of K gaussian components which is equal to the length of weights' vector
  len.y <- length(y) # save the number of samples
  
  # set k random colors
  cols <- randomColor(count = k, 
                      hue = c(" ", "random", "red", "orange", "yellow", "green", "blue", "purple", "pink", "monochrome"), 
                      luminosity = c(" ", "random", "light", "bright", "dark"))
  
  # compute the likelihood for k Gaussians
  like <- likelihood(y, p, mu, sigma)
  
  deviance <- -2 * sum(log(like))
  res <- matrix(NA, 1, 3 * k + 2) # the columns are 3*k because 3 are the Gaussian parameters of length k and +2 because we want to save also the number of each iteration and the corresponding deviance
  res[1,] <- c(0, p, mu, sigma, deviance) # store the result for each i-th iteration
  
  d <- matrix(NA, k, len.y)
  r <- matrix(NA, k, len.y) # initialization of the responsabilities matrix 
  for (iter in 1:n.iter) { # start i-th iteration
    
    # E step
    for(i in 1:k) d[i,] <- p[i]*dnorm(y, mu[i], sigma[i])
    for(i in 1:k) r[i,] <- d[i,] / colSums(d) # compute the responsabilites for each observation
    
    # M step - update the theta parameters for the Gaussian components
    for(i in 1:k) {
      p[i] <- mean(r[i,])
      mu[i] <- sum(r[i,] * y) / sum(r[i,])
      sigma[i] <- sqrt(sum(r[i,] * (y^2)) / sum(r[i,]) - (mu[i])^2)
    }
    
    # check if any parameter collapsed to NA or NaN
    if(any(!is.finite(p)) || any(!is.finite(mu)) || any(!is.finite(sigma))) { 
      # restore the parameter's values of the previous iteration
      p <- res[iter, 2:(k+1)]
      mu <- res[iter, (k+2):(2*k+1)]
      sigma <- res[iter, (2*k+2):(3*k+1)]
      break # exit from the loop
    }
    
    # heuristic to try to avoid singularities, as pointed out in reference [1]
    collapsing <- (sigma <= 0.01) # mask to detect variance collapsing to 0
    sigma[collapsing] <- runif(1, min = 1, max = 2) # set sigma[i] to a non-zero value
    mu[collapsing] <- rnorm(n = 1, mean = mean(y), sd = sd(y))  # in this case, the collapsed to a point of the
                                                                # training set; set the mean to a random value
    # -2 x log-likelihood (a.k.a. deviance)
    like <- likelihood(y, p, mu, sigma)
    deviance.pred <- deviance
    deviance <- -2 * sum( log(like) )
    
    # Save the results of the i-th iteration
    res <- rbind(res, c(iter, p, mu, sigma, deviance))
    
    # check convergence
    if(abs(deviance - deviance.pred) <= tol) {  
      # if the change of the deviance is no more than the tolerance threshold, then EM converged 
      break # exit from the loop
    }
    
    # Plot
    if (plot.flag){
      hist(y, prob = T, breaks = 50, col = gray(.8), border = NA, 
           main = "", xlab = paste("EM Iteration: ", iter, "/", n.iter, sep = ""))
      set.seed(123)
      
      for(i in 1:k) d[i,] <- p[i]*dnorm(y, mu[i], sigma[i]) # update d 
      
      points(jitter(y), rep(0,length(y)), 
             pch = 19, cex = .6, 
             col = assign.color(cols, d)) # assigns the colors to the observation point
      curve(likelihood(x, p, mu, sigma),
            lwd = 4, col = rgb(0,0,0,.5), add = TRUE)
      Sys.sleep(1)
    }
  }
  res <- data.frame(res)
  names(res) <- c("iteration", paste("p", 1:k, sep=""), paste("mu", 1:k, sep=""), paste("sigma", 1:k, sep=""), "deviance") # generalize the number of columns with k components parameters
  out <- list(parameters = c(p = p, mu = mu, sigma = sigma), deviance = deviance, res = res)
  return(out) # return the final result
}

```

```{r include=FALSE}
initialize.parameters <- function(y, k) {
  mean <- mean(y) # mean of the training set
  sd <- sd(y) # standard deviation of the training set
  p <- c(rep(1/k, k)) # equally weight the k components
  mu <- rnorm(n = k, mean = mean, sd = sd) # generate random k means parameters
  sigma <- abs(rnorm(n = k, mean = mean, sd = sd)) # generate random k sigma parameters
  
  return(list(p = p, mu = mu, sigma = sigma)) # return the list of generated parameters 
}
```

```{r message=FALSE, warning=FALSE, include=FALSE}
require(mixtools) # import the library in order to generate the MoG

bart.samples <- function(n) {
  return( rnormmix(n,
                   lambda = c(0.5, rep(0.1,5)),
                   mu = c(0, ((0:4)/2)-1),
                   sigma = c(1, rep(0.1,5))) )
}
```

```{r include=FALSE}
plot.curve <- function(y, p, mu, sigma) { # useful to plot the final result iteration
  cols <- randomColor(count = length(p), 
                    hue = c(" ", "random", "red", "orange", "yellow", "green", "blue", "purple", "pink", "monochrome"), 
                    luminosity = c(" ", "random", "light", "bright", "dark"))
  
  k <- length(p)
  d <- matrix(NA, k, length(y))
  for(i in 1:k) d[i,] <- p[i]*dnorm(y, mu[i], sigma[i]) # compute d 
    
  hist(y, prob = T, breaks = 50, col = gray(.8), border = NA, 
     main = "Final likelihood")
  set.seed(123)
  points(jitter(y), rep(0,length(y)), 
         pch = 19, cex = .6, 
         col = assign.color(cols, d))
  curve(likelihood(x, p, mu, sigma),
        lwd = 4, col = rgb(0,0,0,.5), add = TRUE)
}
```

$$~$$
# Test EM-fit with different sample sizes

As mentioned at the beginning, we sample from a Bart distribution, which is defined as:

$$p(x) = \frac{1}{2} \phi(x|0,1)+\frac{1}{10}\sum_{j=0}^4\phi\left(x|\frac{j}{2}-1, \frac{1}{10}\right),$$
where $\phi(x|\mu,\sigma)$ denotes a Normal density with mean $\mu$ and standard deviation $\sigma$.

We run the simulation on two different sample sizes. In particular, we pick $n_1 << n_2$, in order to study the behavior of the algorithm in a clearly non-asymptotic regime and in a reasonably asymptotic one.


We initialize the parameters of the MoG as follows:

$$
p^{(0)}_i = \frac{1}{k},\\
\mu^{(0)}_i \sim N(\mu_s, \sigma_s^2),\\
\sigma^{(0)}_i \sim N(\mu_s, \sigma_s^2),\\
$$
where $k$ is the number of Gaussians, $\mu_s, \sigma_s^2$ are the sample mean and variance, respectively.

$$~$$

## Non-Asymptotic regime

Before going further, we test the functions defined so far with $n_1 = 40$: 

```{r}
n1 <- 40 # Sample size that follows a non-asymptotic way
XX <- bart.samples(n1)

parameters <- initialize.parameters(XX, k=6) # k is equal to six according to the Bart distribution

#extract the corresponding gaussian parameters
p <- parameters$p 
mu <- parameters$mu 
sigma <- parameters$sigma
iter <- 1000
# executed the handmade.em function for twenty iterations
out1 <- handmade.em(XX, p, mu, sigma, n.iter = iter, plot.flag = F)
plot.curve(XX, out1$parameters[1:6], out1$parameters[7:12], out1$parameters[13:18], n.iter = iter)
```

$$~$$

## Asymptotic regime

...and with $n_2 = 1000$:

```{r}
n2 <- 1000 # Sample size that follows a reasonably asymptotic way
XX <- bart.samples(n2)

parameters <- initialize.parameters(XX, k=6) # k is equal to six according to the bart distribution

#extract the corresponding gaussian parameters
p <- parameters$p 
mu <- parameters$mu 
sigma <- parameters$sigma
iter <- 1000
# executed the handmade.em function for twenty iterations
out2 <- handmade.em(XX, p, mu, sigma, n.iter = iter, plot.flag = F)
plot.curve(XX, out2$parameters[1:6], out2$parameters[7:12], out2$parameters[13:18], n.iter = iter)
```

$$~$$

## The reasons of these results?

Even with these naive examples, our initial expectations are confirmed. Indeed, according to the Law of Large Numbers and the Central Limit Theorem the number of samples are significant in order to follow the true parameters of the distribution. The LLN says that we can achieve the true mean of the distribution increasing the number of samples and with the CLT we can see if the distribution follows the Gaussian shape.

$$~$$

# Model Selection

In this section we introduce different methods in order to find the best $k$ components for these different sample sizes distributed as a Bart density.

## AIC (Akaike Information Criterion) & BIC (Bayesian Information Criterion)

The AIC method is used for the model selection, it adds a penalization proportional to the complexity of the models: the more complex the model is the more relevant is the penalization.

$$AIC = 2 \cdot l_j(\hat\theta_j) - 2 \cdot dim(\Theta_j)$$

where:

- $\hat\theta$  is the vector of estimated parameters
- $l_j$ is the likelihood function
- $dim(\Theta)$ is the number of the model parameters

...as AIC, the BIC method is used for the model selection. It penalizes more the model when the model complexity increases.

$$BIC = l_j(\hat\theta_j) - \frac{\log(n)}{2} \cdot dim(\Theta_j)$$

where:

- $\hat\theta$  is the vector of estimated parameters
- $l_j$ is the likelihood function
- $dim(\Theta)$ is the number of the model parameters
- $n$ is the sample size

As defined below, we implement a general function in order to choose which penalty apply to the model:

$$~$$

```{r}
get.AIC.BIC <- function(y, kmax, n.iter) {
  len.y <- length(y);
  aic.score <- c(); bic.score <- c()
  
  for (k in 1:kmax){
    parameters <- initialize.parameters(y, k) # randomly initialize parameters
    p <- parameters$p
    mu <- parameters$mu
    sigma <- parameters$sigma 
    
    out.opt <- handmade.em(y, p, mu, sigma, n.iter = n.iter, plot.flag = F)$parameters # get the estimated parameters
    
    p <- out.opt[1:k]
    mu <- out.opt[(k+1):(2*k)]
    sigma <- out.opt[(2*k+1):(3*k)]
      
    like <- likelihood(y, p, mu, sigma) # calculate the likelihood with the previous estimated parameters
  
    aic.score <- c(aic.score, 2 * sum(log(like)) - 2 * (3*k-1)) # calculate the AIC
    bic.score <- c(bic.score, sum(log(like)) - log(len.y)/2 * (3*k-1)) # calculate the BIC
  }
  res <- list()
  res[["best_AIC"]] <- which.max(aic.score) # return the index of the k component the maximize the score 
  res[["best_BIC"]] <- which.max(bic.score) # return the index of the k component the maximize the score 
  return(res)
}
```

$$~$$

## Sample Splittings

This particular method divides randomly the main dataset into training and test set given a percentage of these splits. After that, we estimate the model parameters using the training set and we evaluate the "goodness" of the estimations using the test set.

$$~$$

```{r}
split.data <- function(y, perc) {
  size.s <- floor(perc * length(y)) # catch the number of observations at x%
  trainIndex <- sample(seq_len(length(y)), size = size.s) # randomly selection of n observations
  return (trainIndex)
}

training.model <- function(y.train, y.test, kmax, n.iter) {
  like.j <- c()
  
  for (k in 1:kmax){
    parameters <- initialize.parameters(y.train, k) # randomly initialize parameters
    p <- parameters$p 
    mu <- parameters$mu 
    sigma <- parameters$sigma
    
    out.opt <- handmade.em(y.train, p, mu, sigma, n.iter = n.iter, plot.flag = F)$parameters # get the estimated parameters
    
    p <- out.opt[1:k]
    mu <- out.opt[(k+1):(2*k)]
    sigma <- out.opt[(2*k+1):(3*k)]
    
    like <- likelihood(y.test, p, mu, sigma) # calculate the likelihood with the previous estimated parameters
    
    like.j <- c(like.j, sum(log(like)))
  }
  
  best.k <- which.max(like.j) # return the index of the k component the maximize the score 
  return(best.k)
}
```

$$~$$

## K-Fold Cross-Validation

This method randomly divides the observations into `k` folds of approximately equal size. At each step, this method chooses one fold as test set and the other ones are used as training set in order to fit the model.

$$~$$

```{r message=FALSE, warning=FALSE}
library(caret) # import the dataset in order to crate the foldes

k.fold.cross.validation <- function(y, k.folds, kmax, n.iter) {
  best.cross.v <- c() 
  expectations <- c()
  like.j <- c()
  
  lst.k.folds <- createFolds(y, k = k.folds, list = TRUE, returnTrain = FALSE) # create k folds with n observations
  
  fold.names <- names(lst.k.folds)
  for (k in 1:kmax) { # train the model kmax iterations
    for (x in 1:k.folds) { # train each fold k times
      # divide the observations
      n.k.fold <- fold.names[x]
      
      test.set <- y[unlist(lst.k.folds[n.k.fold], use.names=FALSE)]            
      train.set <- y[-unlist(lst.k.folds[n.k.fold], use.names = FALSE)]          
      
      # randomly initialize parameters
      parameters <- initialize.parameters(train.set, k) 
      p <- parameters$p 
      mu <- parameters$mu 
      sigma <- parameters$sigma
      
      # get the estimated parameters
      out.opt <- handmade.em(train.set, p, mu, sigma, n.iter = n.iter, plot.flag = F)$parameters
      
      p <- out.opt[1:k]
      mu <- out.opt[(k+1):(2*k)]
      sigma <- out.opt[(2*k+1):(3*k)]
      
      # calculate the likelihood with the previous estimated parameters
      like <- likelihood(test.set, p, mu, sigma)  
      
      # for each fold calculate the mean of the log-likelihood
      like.j <- c(like.j, mean(log(like)))
    }
    
    # save the i-th expectation
    expectations <- c(expectations, mean(like.j))
  }

  best.cross.v <- which.max(expectations) # return the index of the k component the maximize the score
  return(best.cross.v)
}
```

$$~$$

## Wasserstein

The Wasserstein distance is a distance function defined between probability distributions on a given metric space. For our work we evaluate this formula:

$$W_k = \int_0^1 |{F_k^{-1}(z|\hat\theta_{Tr})-\hat F_{Te}^{-1}(z)}| dz$$


where:

- $F_k^{-1}(z|\hat\theta_{Tr})$ is the quantile function of a k components MoG
- $F_{Te}^{-1}(z)$ denotes the quantile function of the true population model

$$~$$

```{r message=FALSE, warning=FALSE}
library(KScorrect) # import this library in order to apply the integrate function

wass.score <- function(y.train, y.test, kmax, n.iter) { # routine function useful to apply the Wasserstein formula
  f <- function(z, p, mu, sigma, y.test) { # internal function to evaluate the integral
    res <- abs(qmixnorm(p = z, mean = mu, sd = sigma, pro = p) - quantile(y.test, probs = z))
    return(res)
  }
  
  wass.sc <- c();
  
  for (k in 1:kmax){
    parameters <- initialize.parameters(y.train, k) # randomly initialize parameters
    p <- parameters$p 
    mu <- parameters$mu 
    sigma <- parameters$sigma
    
    out.opt <- handmade.em(y.train, p, mu, sigma, n.iter = n.iter, plot.flag = F)$parameters # get the estimated parameters
    
    p <- out.opt[1:k]
    mu <- out.opt[(k+1):(2*k)]
    sigma <- out.opt[(2*k+1):(3*k)]
    
    integral <- tryCatch(
                        integrate(f, lower = 0, upper = 1,
                        p = p, mu = mu, sigma = sigma, y.test = y.test, 
                        rel.tol=.Machine$double.eps^.05)$value, # execute the integral in order to have the score
                        error = function(e) return(NA)
                        )
    wass.sc <- c(wass.sc, integral)
  }
  
  return(wass.sc)
}
```

$$~$$

# ...Do Simulations!

We define a routine function in order to simulate M=100 times the samples from a Bart distribution. 

$$~$$

```{r}
do.simulate <- function(n, M, n.iter, kmax) {
  max.k <- function(res) { # return the max indexes of k components
    k <- 0
    max <- -1
    for(i in 1:length(res)) {
      if(max <= res[i] && k < i) {
        k <- i
        max <- res[i]
      }
    }
    return(k)
  }

  #initialization of parameters
  best.score50and50 <- c(); best.score70and30 <- c(); best.score30and70 <- c();
  best.aic.k <- c(); best.bic.k <- c();
  best.cross.validation5 <- c(); best.cross.validation10 <- c();
  
  wasses <- c(); 
  
  wasses.scores <- data.frame(matrix(ncol=2,nrow=0, dimnames=list(NULL, c("k.component", "score"))))
  
  # M simulations starting now..
  for (i in 1:M) {
    XX <- bart.samples(n)
    
    # find the best k components with AIC
    res.AIC.BIC <- get.AIC.BIC(XX, kmax = kmax, n.iter = n.iter)
    best.aic.k <- c(best.aic.k, res.AIC.BIC[["best_AIC"]])
    
    best.bic.k <- c(best.bic.k, res.AIC.BIC[["best_BIC"]])
    
    # 50% Training-Test
    indexes50.50 <- split.data(XX, perc=0.5)
    XX.Train50 <- XX[indexes50.50]
    XX.Test50 <- XX[-indexes50.50]
    
    best.score50and50 <- c(best.score50and50, training.model(XX.Train50, XX.Test50, kmax=kmax, n.iter))
    
    # 70%-30% Training-Test
    indexes70.30 <- split.data(XX, perc=0.7)
    XX.Train70 <- XX[indexes70.30]
    XX.Test30 <- XX[-indexes70.30]
    
    best.score70and30 <- c(best.score70and30, training.model(XX.Train70, XX.Test30, kmax=kmax, n.iter))
    
    # 30%-70% Training-Test
    indexes30.70 <- split.data(XX, perc=0.3)
    XX.Train30 <- XX[indexes30.70]
    XX.Test70 <- XX[-indexes30.70]
    
    best.score30and70 <- c(best.score30and70, training.model(XX.Train30, XX.Test70, kmax=kmax, n.iter))
    
    # K-Fold Cross-Validation
    best.cross.validation5 <- c(best.cross.validation5, k.fold.cross.validation(XX, k.folds=5, kmax=kmax, n.iter))
    best.cross.validation10 <- c(best.cross.validation10, k.fold.cross.validation(XX, k.folds=10, kmax=kmax, n.iter))
    
    #Wass score
    w <- wass.score(XX.Train50, XX.Test50, kmax=kmax, n.iter)
    wasses.scores <- rbind(wasses.scores, data.frame( k.component = 1:kmax, 
                                                      score = w ) )
    wasses <- c(wasses, which.min(w))
  }
  
  return(list(  AIC = best.aic.k, 
                BIC = best.bic.k,
                Split50_50 = best.score50and50,
                Split70_30 = best.score70and30,
                Split30_70 = best.score30and70,
                five_fold_cv = best.cross.validation5,
                ten_fold_cv = best.cross.validation10,
                wass_score_k = wasses,
                wasses_scores = wasses.scores
              )
         )

}
```

$$~$$

Start to simulate with $n_1$ and $n_2$ samples size defined above.

```{r echo=FALSE, warning=FALSE}
kmax <- 8
n.iter <- 200

# try to simulate with n1 = 40 
res.n1 <- do.simulate(n1, M=100, n.iter=n.iter, kmax=kmax)

# try to simulate with n2 = 1000 
res.n2 <- do.simulate(n2, M=100, n.iter=n.iter, kmax=kmax)
```

$$~$$

# Plot & Summaries

After the simulations, we plot the frequencies of the $k$ values selected by each method: 

$$~$$

```{r include=FALSE}
require(zoo)

plot.AIC.BIC <- function(res, n) {
  freq.aic <- tabulate(res[["AIC"]])
  freq.bic <- tabulate(res[["BIC"]])
  plot(1:length(freq.aic), freq.aic,
       main = paste("K distribution with ", n, " samples across M=100 simulations", sep=""), xlab="k components", ylab="Frequency",
       lty=3, lwd = 3, type = "b", pch = 21, col="red", xlim = c(1, kmax), ylim = c(1, 100))
  lines(1:length(freq.bic), freq.bic,
        lty=3, lwd = 3, type = "b", pch = 21, col="blue")
  legend(7, 95, legend=c("AIC", "BIC"),
         col=c("red", "blue"), lty=2, cex=0.8) 
}

plot.Splittings <- function(res, n) {
  freq.Split50_50 <- tabulate(res[["Split50_50"]])
  freq.Split70_30 <- tabulate(res[["Split70_30"]])
  freq.Split30_70 <- tabulate(res[["Split30_70"]])
  
  plot(1:length(freq.Split50_50), freq.Split50_50,
       main = paste("K distribution with ", n, " samples across M=100 simulations", sep=""), xlab="k components", ylab="Frequency",
       lty=3, lwd = 3, type = "b", pch = 21, col="red", xlim = c(1, kmax), ylim = c(1, 100))
  lines(1:length(freq.Split70_30), freq.Split70_30,
       lty=3, lwd = 3, type = "b", pch = 21, col="blue")
  lines(1:length(freq.Split30_70), freq.Split30_70,
       lty=3, lwd = 3, type = "b", pch = 21, col="green")
  legend(6.3, 95, legend=c("Split 50-50%", "Split 70-30%", "Split 30-70%"),
         col=c("red", "blue", "green"), lty=2, cex=0.8)
}

plot.kfolds <- function(res, n) {
  freq.five_fold_cv <- tabulate(res[["five_fold_cv"]])
  freq.ten_fold_cv <- tabulate(res[["ten_fold_cv"]])
  
  plot(1:length(freq.five_fold_cv), freq.five_fold_cv,
       main = paste("K distribution with ", n, " samples across M=100 simulations", sep=""), xlab="k components", ylab="Frequency",
       lty=3, lwd = 3, type = "b", pch = 21, col="red", xlim = c(1, kmax), ylim = c(1, 100))
  lines(1:length(freq.ten_fold_cv), freq.ten_fold_cv,
       lty=3, lwd = 3, type = "b", pch = 21, col="blue")
  legend(6.5, 95, legend=c("5-Fold", "10-Fold"),
         col=c("red", "blue"), lty=2, cex=0.8)  
}

plot.wass <- function(res, n) {
  freq.wass <- tabulate(res[["wass_score_k"]])
  
  plot(1:length(freq.wass), freq.wass,
       main = paste("K distribution with ", n, " samples across M=100 simulations", sep=""), xlab="k components", ylab="Frequency Wass Score",
       lty=3, lwd = 3, type = "b", pch = 21, col="red", xlim = c(1, kmax), ylim = c(1, 50))
}

violin.plot <- function(res, n) {
  res$wasses_scores <- res$wasses_scores[res$wasses_scores$k.component != 1,]
  # fill NA with the scores' mean of the corresponding k
  res$wasses_scores$score <- na.aggregate(res$wasses_scores$score, by = res$wasses_scores$k.component) 
  res$wasses_scores$k.component <- as.factor(res$wasses_scores$k.component)
  p <- ggplot(res$wasses_scores, aes(x=k.component, y=score)) + geom_violin(trim=FALSE, aes(fill=k.component)) +
       geom_boxplot(width=0.1) + 
       labs(title=paste("Plot of k-component by scores with ", n, " samples"),x="k component", y = "scores") + 
       scale_fill_brewer(palette="Dark2") + 
       theme_minimal()
  p 
}
```

```{r}
plot.AIC.BIC(res.n1, "n1")
plot.Splittings(res.n1, "n1")
plot.kfolds(res.n1, "n1")
plot.wass(res.n1, "n1")
```

$$~$$
As we can see in the plots, taking $n_1$ all of the methods select the lowest $k$, since the number of samples is too small and then we are considering a non asymptotic regime.

$$~$$

```{r}
plot.AIC.BIC(res.n2, "n2")
plot.Splittings(res.n2, "n2")
plot.kfolds(res.n2, "n2")
plot.wass(res.n2, "n2")
```

$$~$$

With $n_2$, the different behaviours of the model selection methods are evident:
- **AIC vs BIC**: in this plot we can see that AIC penalizes less the model as it gets more complex, then it usually selects a number of $k$ greater than that selected by BIC. In both cases, the selected $k$ is not equal to the real $k^*=6$ of the Bart, even though the number of times in which they pick $k=6$ is high.
- **Train-Test splits**: as expected, the good proportions for the splitting are 50-50% and 70-30%, while the performance of 30-70% is very poor.
- **k-folds**: surprisingly, the value of $k$ selected usually coincides with the value of $k_{max}$.
- **Wasserstein**: the selcted $k$ is near to $k*$.

$$~$$

We then plot the Wasserstein distances across the M iterations, using a violin plot. Since, as expected, with $k=1$ and $n_2$ there are many outliers, we keep the case of $k=1$ out. This choice doesn't affect the results, since $k=1$ is not a significant case.  

```{r warning=FALSE}
violin.plot(res.n1, "n1")
violin.plot(res.n2, "n2")
```

$$~$$

Then, we see which metric has selected the more times the true $k_{max}$ for the Bart distribution, considering $k^* = 6$:

```{r include=FALSE}
best.model.prediction <- function(res, n) {
  kmax_true = 6; scores <- c();
  M <- length(res$AIC) # the length of simulations
  for (name in names(res)) {
    if (name!="wasses_scores") {
      perc.pred <- sum(res[[name]]==kmax_true) / M # this ratio represents the number of corrected classification through all M simulations
      scores <- c(scores, perc.pred)
      
      print(paste(name, ": ", perc.pred))
    }
  }
  
  return(paste("----> The best model for predicting the true kmax with", n, "samples is: ", names(res)[which.max(scores)]))
}

print(best.model.prediction(res.n1, "n1"))
print(best.model.prediction(res.n2, "n2"))
```

$$~$$

We display the through the iterations of six gaussian parameters the changment of the parameters at each iteration:

```{r include=FALSE}
plot.it <- function(param, type, n.iter) {
  cols <- c()
  cols <- c(cols, randomColor(count = 1,
                          hue = c(" ", "random", "red", "orange", "yellow", "green", "blue", "purple", "pink", "monochrome"),
                          luminosity = c(" ", "random", "light", "bright", "dark")))
  if(type!="p"){
    plot(c(1:(n.iter+1)), param[1][[colnames(param)[1]]],
       main = paste(type, " at ", n.iter, "+1 iterations", sep=""), xlab="itereration", ylab="values",
       lty=1, lwd = 3, type = "l", pch = 21, col=cols[1], xlim = c(1, n.iter), ylim = c(-2, 2), xaxs="i", yaxs="i")
  } else {
      plot(c(1:(n.iter+1)), param[1][[colnames(param)[1]]],
       main = paste(type, " at ", n.iter, "+1 iterations", sep=""), xlab="itereration", ylab="values",
       lty=1, lwd = 3, type = "l", pch = 21, col=cols[1], xlim = c(1, n.iter), ylim = c(0, 1), xaxs="i", yaxs="i")
  }

  for(i in 2:6) {
     cols <- c(cols, randomColor(count = 1,
                          hue = c(" ", "random", "red", "orange", "yellow", "green", "blue", "purple", "pink", "monochrome"),
                          luminosity = c(" ", "random", "light", "bright", "dark")))
     lines(c(1:(n.iter+1)), param[i][[colnames(param)[i]]],
        lty=1, lwd = 3, type = "l", pch = 21,
        col = cols[i])
  }
  
  legend("bottomright", legend=colnames(param), col=cols, lty=1, cex=0.8)   
}

plot.params <- function(y, n.iter, kmax=6) {
  out <- c(rep(0, kmax));

  parameters <- initialize.parameters(y, 6) # randomly initialize parameters
  p <- parameters$p 
  mu <- parameters$mu 
  sigma <- parameters$sigma
  
  out <- handmade.em(y, p, mu, sigma, n.iter = n.iter, plot.flag = F) # get the estimated

  hist.values <- out$res
  
  opt.p <- hist.values[2:7]
  opt.mu <- hist.values[8:13]
  opt.sigma <- hist.values[14:19]
  
  plot.it(opt.p, "p", n.iter)
  plot.it(opt.mu, "mu", n.iter)
  plot.it(opt.sigma, "sigma", n.iter)
}
```

```{r}
plot.params(bart.samples(n1), 100, kmax=6)
plot.params(bart.samples(n2), 100, kmax=6)
```

Finally, knowing that, in general, if $\hat\theta_r$ is the estimeted $r^{th}$ parameter and $\theta_r$ the $r^{th}$ true parameter, $\frac{\hat\theta_r - \theta_r}{\hat{se}_r} \xrightarrow{d} N(0,1)$, we plot the empirical distribution of the estimated weights $\hat p_r$: we expect they follow a Normal distro with mean $p_r$ and a certain $\hat{se}_r$:

$$~$$

```{r include=FALSE}
check.p.distro <- function(n, M = 100) {
  p.s <- matrix(0, 6, M)
  for(i in 1:M) {
    XX <- bart.samples(n)
    parameters <- initialize.parameters(XX, 6) # randomly initialize parameters
    p <- parameters$p 
    mu <- parameters$mu 
    sigma <- parameters$sigma
    
    out.em <<- handmade.em(XX, p, mu, sigma, n.iter = 200, plot.flag = F)
    p.s[1, i] <- out.em$parameters[1]
    p.s[2, i] <- out.em$parameters[2]
    p.s[3, i] <- out.em$parameters[3]
    p.s[4, i] <- out.em$parameters[4]
    p.s[5, i] <- out.em$parameters[5]
    p.s[6, i] <- out.em$parameters[6]
  }
  return(p.s)
}
```

```{r}
out <- check.p.distro(n = 1000, M = 1000)
for(i in 1:6) {
  hist(out[i,], col = "orchid", border = "white", 
       main = paste("Parameter p", i, sep=""), 
       xlab = paste("p", i, sep=""), xlim = c(0,1)
  )
}
```
$$~$$

As the plots show, the distribution of each $p_r$ in some cases approximately has a Gaussian shape, even though the means don't coincide with the true Bart's weights. Probably this is due to the choices made during the initialization.
